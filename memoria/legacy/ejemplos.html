<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Operaciones comunes en NLP</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            background-color: #f0f2f5;
        }
        .container {
            width: 60%;
            max-width: 800px;
            margin: 20px auto;
            background: #ffffff;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        pre {
            background-color: #282c34;
            color: #abb2bf;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            border: 1px solid #ddd;
        }
        code {
            font-family: "Courier New", Courier, monospace;
            color: #61dafb;
        }
        h1, h2, h3 {
            text-align: center;
            color: #333;
        }
        p {
            text-align: justify;
            line-height: 1.6;
            color: #555;
        }
        hr {
            border: 1px solid #ddd;
        }
        #indice {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        #indice ul {
            list-style-type: none;
            padding-left: 0;
        }
        #indice ul ul {
            padding-left: 20px;
        }
        #indice a {
            text-decoration: none;
            color: #007bff;
            transition: color 0.3s ease;
        }
        #indice a:hover {
            color: #0056b3;
            text-decoration: underline;
        }
        .titulo-indice {
            border-bottom: 2px solid #007bff;
            padding-bottom: 10px;
            margin-bottom: 15px;
        }
        .lista-indice > li {
            margin-bottom: 10px;
        }
        .sublista-indice {
            margin-top: 5px;
        }
        .enlace-indice {
            font-weight: bold;
        }
        .subenlace-indice {
            font-style: italic;
        }
        .highlight {
            background-color: #fffacd;
            color: #333;
            padding: 2px 4px;
            border-radius: 3px;
        }
    </style>
</head>
<body>

<div class="container">
    <h1 id="titulo-principal">Operaciones comunes en NLP</h1>
    <div id="indice" class="indice-elegante">
        <h2 class="titulo-indice">Índice</h2>
        <nav>
            <ul class="lista-indice">
                <li><a href="#titulo-principal" class="enlace-indice">Operaciones comunes en NLP</a></li>
                <li>
                    <a href="#funciones-perdida-prediccion" class="enlace-indice">Funciones de pérdida y predicción en NLP</a>
                    <ul class="sublista-indice">
                        <li><a href="#funcion-perdida" class="subenlace-indice">Función de pérdida</a></li>
                        <li><a href="#funcion-prediccion" class="subenlace-indice">Función de predicción</a></li>
                    </ul>
                </li>
                <li><a href="#entrenamiento-red-neuronal" class="enlace-indice">Entrenamiento de una Red Neuronal para Clasificación</a></li>
                <li><a href="#ejemplos-codigo" class="enlace-indice">Ejemplos de código</a></li>
            </ul>
        </nav>
    </div>

    <p>A continuación hay ejemplos de operaciones comunes entre tensores/matrices en el ámbito de procesamiento natural:</p>

    <h2 id="funciones-perdida-prediccion">Funciones de pérdida y predicción en NLP</h2>
    
    <p>En el procesamiento de lenguaje natural (NLP), las funciones de pérdida y predicción son fundamentales para entrenar y evaluar modelos. Estas funciones implican operaciones complejas entre matrices y tensores para calcular el rendimiento del modelo y hacer predicciones.</p>
    
    <h3 id="funcion-perdida">Función de pérdida</h3>
    <p>La función de pérdida calcula cuán bien está funcionando el modelo. Implica operaciones matriciales entre las predicciones del modelo y las etiquetas reales.</p>
    
    <p>Posibles errores de dimensionalidad:</p>
    
    <ul>
        <li><strong>En la entrada de la red:</strong> Asegúrese de que <span class="highlight">xs_bow</span> tenga la forma esperada por la red neuronal.</li>
        <li><strong>En el cálculo de pérdida:</strong> Verifique que <span class="highlight">logits</span> y <span class="highlight">labels</span> tengan las dimensiones correctas para <span class="highlight">criterion</span>.</li>
        <li><strong>En las operaciones escalares:</strong> Asegúrese de que <span class="highlight">xs_bow.shape[0]</span> sea consistente con el tamaño del batch.</li>
    </ul>
    
    <pre><code class="language-python">def get_loss(net, iterator, criterion):
    net.eval()
    total_loss = 0
    num_evals = 0
    with torch.no_grad():
        for xs_bow, labels in iterator:
            # Operación de transferencia de tensor a GPU
            xs_bow, labels = xs_bow.cuda(), labels.cuda()

            # Operación de multiplicación de matrices para obtener logits
            logits = net(xs_bow)

            # Operación de cálculo de pérdida entre tensores
            loss = criterion(logits, labels)

            # Operaciones escalares y de forma de tensor
            total_loss += loss.item() * xs_bow.shape[0]
            num_evals += xs_bow.shape[0]

    # Operación escalar final
    return total_loss / num_evals
    </code></pre>

    <h3 id="funcion-prediccion">Función de predicción</h3>
    <p>La función de predicción utiliza el modelo entrenado para hacer predicciones sobre nuevos datos. Implica operaciones de multiplicación de matrices y transformaciones de tensores.</p>

    <p>Posibles errores de dimensionalidad:</p>
    
    <ul>
        <li><strong>En la entrada de la red:</strong> Asegúrese de que <span class="highlight">xs_bow</span> tenga la forma esperada por la red neuronal.</li>
        <li><strong>En la activación sigmoid:</strong> Verifique que <span class="highlight">logits</span> tenga la forma correcta para la operación sigmoid elemento a elemento.</li>
        <li><strong>En la operación argmax:</strong> Asegúrese de que <span class="highlight">soft_probs</span> tenga la dimensión correcta para realizar argmax en el eje 1.</li>
        <li><strong>En la conversión final:</strong> Verifique que <span class="highlight">preds</span> y <span class="highlight">tests</span> tengan la misma longitud antes de convertirlos a arrays numpy.</li>
    </ul>

    <pre><code class="language-python">def get_preds_tests_nn(net, iterator):
    net.eval()
    preds, tests = [], []
    with torch.no_grad():
        for xs_bow, labels in iterator:
            # Operación de transferencia de tensor a GPU
            xs_bow, labels = xs_bow.cuda(), labels.cuda()

            # Operación de multiplicación de matrices para obtener logits
            logits = net(xs_bow)

            # Operación de activación sigmoid elemento a elemento
            soft_probs = nn.Sigmoid()(logits)

            # Operaciones de conversión de tensor a lista y argmax
            preds += np.argmax(soft_probs.tolist(), axis=1).tolist()
            tests += labels.tolist()

    # Operación de conversión de lista a array numpy
    return np.array(preds), np.array(tests)
    </code></pre>
    <h2 id="entrenamiento-red-neuronal">Entrenamiento de una Red Neuronal para Clasificación</h2>
    
    <p>Este código implementa el entrenamiento de una red neuronal para una tarea de clasificación. A continuación, se describe el proceso y se destacan posibles errores de dimensionalidad de matrices:</p>
    
    <ol>
        <li>Se definen los parámetros de la red, incluyendo dimensiones y hiperparámetros.</li>
        <li>Se inicializa la red neuronal, el criterio de pérdida y el optimizador.</li>
        <li>Se implementa el ciclo de entrenamiento por épocas.</li>
    </ol>
    
    <p>Posibles errores de dimensionalidad:</p>
    
    <ul>
        <li><strong>En la inicialización de la red:</strong> Asegúrese de que <span class="highlight">dim_vocab</span> coincida con la dimensión de entrada de su red.</li>
        <li><strong>En el forward pass:</strong> Verifique que la salida de <span class="highlight">net(xs_bow)</span> tenga la forma esperada por <span class="highlight">criterion</span>.</li>
        <li><strong>En el cálculo de pérdida:</strong> Asegúrese de que <span class="highlight">logits</span> y <span class="highlight">preds</span> tengan las dimensiones correctas para <span class="highlight">criterion</span>.</li>
        <li><strong>En las funciones <span class="highlight">get_loss</span> y <span class="highlight">get_preds_tests_nn</span>:</strong> Verifique que estas funciones manejen correctamente las dimensiones de los tensores de entrada y salida.</li>
    </ul>
    <pre><code class="language-python">
    import torch.optim as optim

    params = {
        "dim_vocab": len(train_loader.dataset.bow_cols),
        "num_classes": 3,
        "dim_hidden_input": 5,
        "dim_hidden_output": 5,
        "learning_rate": 0.4,
        "epochs": 15
    }
    
    # Inicialice su red neuronal
    net = MyNeuralNetwork(
        dim_vocab=params["dim_vocab"],
        num_classes=params["num_classes"],
        dim_hidden_input=params["dim_hidden_input"],
        dim_hidden_output=params["dim_hidden_output"]).cuda()
    
    # Definir la Loss = Cross-entropy
    criterion = nn.CrossEntropyLoss().cuda()
    
    # Definir el optimizador = SGD: Stochastic-gradient Descent
    opti = optim.SGD(net.parameters(), lr = params["learning_rate"])
    
    # Definir el numero de epocas de entrenamiento
    epochs = params["epochs"]
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    ## Implementar desde aqui el ciclo de entrenamiento
    ## para cada epoca en el conjunto de train
    for epoch in range(epochs):
      for (xs_bow, labels) in train_loader:
    
        opti.zero_grad()
        xs_bow, preds = xs_bow.to(device), labels.to(device)
        logits = net(xs_bow)
        loss = criterion(logits, preds)
        loss.backward()
        opti.step()
    
      total_loss = get_loss(net, train_loader, criterion)
      y_preds, y_tests = get_preds_tests_nn(net, train_loader)
      acc = (y_preds == y_tests).sum() / y_preds.shape[0]
    
      print("Epoca {} completada! Loss: {} Accuracy: {}".format(epoch, total_loss, acc))
    </code></pre>


</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-python.min.js"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-okaidia.min.css">

</body>
</html>
